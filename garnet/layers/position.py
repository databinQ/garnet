# coding: utf-8

"""
@File   : position.py
@Author : garnet
@Time   : 2020/4/13 16:56
"""

import keras
import keras.backend as K


class FixedPositionEmbedding(keras.layers.Layer):
    """
    Fixed position embedding layer. Embedding weights are generated by `sin` and `cos` function, and can not be trained
    during training process.
    """

    def __init__(self,
                 hidden_size,
                 mode='add',
                 **kwargs):
        super(FixedPositionEmbedding, self).__init__(**kwargs)
        self.supports_masking = True

        self.hidden_size = hidden_size
        self.mode = mode

    def call(self, inputs, **kwargs):
        seg_index = K.arange(K.shape(inputs)[1])
        seg_index = K.expand_dims(seg_index, 0)
        seg_index = K.tile(seg_index, [K.shape(inputs)[0], 1])  # (batch_size, seg_len)
        p = self.idx2pos(seg_index)
        if self.mode == 'add':
            return inputs + p
        else:
            return K.concatenate([inputs, p])

    def idx2pos(self, idx):
        idx = K.cast(idx, K.floatx())
        idx = K.expand_dims(idx, 2)  # (batch_size, seg_len, 1)
        pj = 1. / K.pow(10000., 2. / self.hidden_size * K.arange(self.hidden_size // 2, dtype=K.floatx()))
        pj = K.expand_dims(pj, 0)  # (1, self.hidden_size // 2)
        pv = K.dot(idx, pj)  # (batch_size, seg_len, self.hidden_size // 2)
        pv1, pv2 = K.sin(pv), K.cos(pv)
        pv1, pv2 = K.expand_dims(pv1, 3), K.expand_dims(pv2, 3)
        pv = K.concatenate([pv1, pv2], 3)  # feature vector is combine by sin and cos by turns
        return K.reshape(pv, (K.shape(pv)[0], K.shape(pv)[1], self.hidden_size))

    def compute_output_shape(self, input_shape):
        if self.mode == 'add':
            return input_shape
        else:
            return input_shape[:-1] + (input_shape[-1] + self.hidden_size,)

    def get_config(self):
        config = {
            'hidden_size': self.hidden_size,
            'mode': self.mode,
        }
        base_config = super(FixedPositionEmbedding, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class PositionEmbedding(keras.layers.Layer):
    """
    Trainable position embedding layer
    """

    def __init__(self,
                 max_seq_length,
                 output_dim,
                 mode='add',
                 embeddings_initializer='zeros',
                 **kwargs):
        """
        :param max_seq_length: Max sequence length, which is equivalent to vacabulary size of embedding
        :param output_dim: Output of embedding
        :param mode: `add` or `mul`
        :param embeddings_initializer: Embeddings initializer, `zeros` as default
        :param kwargs:
        """
        super(PositionEmbedding, self).__init__(**kwargs)
        self.supports_masking = True

        self.max_seq_length = max_seq_length
        self.output_dim = output_dim
        self.mode = mode
        self.embeddings_initializer = keras.initializers.get(embeddings_initializer)

    def build(self, input_shape):
        self.embeddings = self.add_weight(
            name='position_embeddings',
            shape=(self.max_seq_length, self.output_dim),
            initializer=self.embeddings_initializer
        )
        super(PositionEmbedding, self).build(input_shape)

    def call(self, inputs, **kwargs):
        input_shape = K.shape(inputs)
        batch_size, seq_len = input_shape[0], input_shape[1]
        pos_embeddings = self.embeddings[:seq_len]
        pos_embeddings = K.expand_dims(pos_embeddings, 0)
        if self.mode == 'add':
            return inputs + pos_embeddings
        else:
            pos_embeddings = K.tile(pos_embeddings, [batch_size, 1, 1])
            return K.concatenate([inputs, pos_embeddings], axis=-1)

    def compute_output_shape(self, input_shape):
        if self.mode == 'add':
            return input_shape
        else:
            return input_shape[:-1] + (input_shape[-1] + self.output_dim,)

    def get_config(self):
        config = {
            'max_seq_length': self.max_seq_length,
            'output_dim': self.output_dim,
            'mode': self.mode,
            'embeddings_initializer': keras.initializers.serialize(self.embeddings_initializer),
        }
        base_config = super(PositionEmbedding, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


custom_objects = {
    'FixedPositionEmbedding': FixedPositionEmbedding,
    'PositionEmbedding': PositionEmbedding,
}

keras.utils.get_custom_objects().update(custom_objects)
